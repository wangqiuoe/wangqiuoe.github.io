@inproceedings{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  booktitle={Advances in neural information processing systems},
  pages={8572--8583},
  year={2019}
}

@article{novak2019neural,
  title={Neural tangents: Fast and easy infinite neural networks in python},
  author={Novak, Roman and Xiao, Lechao and Hron, Jiri and Lee, Jaehoon and Alemi, Alexander A and Sohl-Dickstein, Jascha and Schoenholz, Samuel S},
  journal={arXiv preprint arXiv:1912.02803},
  year={2019}
}

@article{cho2009kernel,
  title={Kernel methods for deep learning},
  author={Cho, Youngmin and Saul, Lawrence},
  journal={Advances in neural information processing systems},
  volume={22},
  pages={342--350},
  year={2009}
}

@article{liu2020gaussian,
  title={When Gaussian process meets big data: A review of scalable GPs},
  author={Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2020},
  publisher={IEEE}
}

@inproceedings{wang2019exact,
  title={Exact Gaussian processes on a million data points},
  author={Wang, Ke and Pleiss, Geoff and Gardner, Jacob and Tyree, Stephen and Weinberger, Kilian Q and Wilson, Andrew Gordon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14648--14659},
  year={2019}
}

@article{friedrich2020neuronal,
  title={Neuronal Gaussian Process Regression},
  author={Friedrich, Johannes},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{burt2019rates,
  title={Rates of Convergence for Sparse Variational Gaussian Process Regression},
  author={Burt, David and Rasmussen, Carl Edward and Van Der Wilk, Mark},
  booktitle={International Conference on Machine Learning},
  pages={862--871},
  year={2019}
}


@article{pang2019neural,
  title={Neural-net-induced Gaussian process regression for function approximation and PDE solution},
  author={Pang, Guofei and Yang, Liu and Karniadakis, George Em},
  journal={Journal of Computational Physics},
  volume={384},
  pages={270--288},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{lee2018deep,
  title={Deep Neural Networks as Gaussian Processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{matthews2018gaussian,
  title={Gaussian Process Behaviour in Wide Deep Neural Networks},
  author={Matthews, Alexander G de G and Hron, Jiri and Rowland, Mark and Turner, Richard E and Ghahramani, Zoubin},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@incollection{neal1996priors,
  title={Priors for infinite networks},
  author={Neal, Radford M},
  booktitle={Bayesian Learning for Neural Networks},
  pages={29--53},
  year={1996},
  publisher={Springer}
}

@article{garnelo2018neural,
  title={Neural processes},
  author={Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J and Eslami, SM and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1807.01622},
  year={2018}
}

@book{ross2014introduction,
  title={Introduction to probability models},
  author={Ross, Sheldon M},
  year={2014},
  publisher={Academic press}
}

@book{williams2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams and Christopher KI and Rasmussen, Carl Edward},
  volume={2},
  year={2006},
  publisher={MIT press Cambridge, MA}
}
@online{WinNT,
  author = {Jochen GÃ¶rtler and Rebecca Kehlbeck and Oliver Deussen},
  title = {A Visual Exploration of Gaussian Processes},
  url = {https://www.jgoertler.com/visual-exploration-gaussian-processes/}
}

@online{UCIdataset,
  author = {Unknown},
  title = {UCI public dataset: Combined Cycle Power Plant Data Set},
  url = {http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant}
}

@online{gprcode,
  author = {Qi Wang},
  title = {Gaussian Process Regression code},
  url = {https://github.com/wangqiuoe/gpr.git}
}

%sgpr
@article{quinonero2005unifying,
  title={A unifying view of sparse approximate Gaussian process regression},
  author={Qui{\~n}onero-Candela and Joaquin and Rasmussen and Carl Edward},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={Dec},
  pages={1939--1959},
  year={2005}
}
%repeat experiments
@inproceedings{hernandez2015probabilistic,
  title={Probabilistic backpropagation for scalable learning of bayesian neural networks},
  author={Hern{\'a}ndez-Lobato and Jos{\'e} Miguel and Adams, Ryan},
  booktitle={International Conference on Machine Learning},
  pages={1861--1869},
  year={2015}
}

@article{bui2017unifying,
  title={A unifying framework for Gaussian process pseudo-point approximations using power expectation propagation},
  author={Bui, Thang D and Yan, Josiah and Turner, Richard E},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3649--3720},
  year={2017},
  publisher={JMLR. org}
}

@article{friedrich2020neuronal,
  title={Neuronal Gaussian Process Regression},
  author={Friedrich, Johannes},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}